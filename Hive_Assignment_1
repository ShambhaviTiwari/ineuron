1)Store raw data into hdfs location:
hadoop fs -put sales_order_data.csv /tmp

2)Create a internal hive table "sales_order_csv" which will store csv data sales_order_csv .. make sure to skip header row while creating table
create table sales_order_csv(ORDERNUMBER INT,
QUANTITYORDERED INT,PRICEEACH INT,ORDERLINENUMBER INT,
SALES INT,STATUS STRING,QTR_ID INT,MONTH_ID INT,YEAR_ID INT,
PRODUCTLINE STRING,MSRP INT,PRODUCTCODE STRING,
PHONE INT,CITY STRING,STATE STRING,
POSTALCODE INT,COUNTRY STRING,TERRITORY STRING,
CONTACTLASTNAME STRING,CONTACTFIRSTNAME STRING,DEALSIZE STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
TBLPROPERTIES("skip.header.line.count"="1");

3)Load data from hdfs path into "sales_order_csv" 
LOAD DATA INPATH '/tmp/sales_order_data.csv' INTO TABLE sales_order_csv;

4) Create an internal hive table which will store data in ORC format "sales_order_orc"
Load data from "sales_order_csv" into "sales_order_orc"

create table sales_order_orc(ORDERNUMBER INT,QUANTITYORDERED INT,
PRICEEACH INT,ORDERLINENUMBER INT,SALES INT,STATUS STRING,QTR_ID INT,
MONTH_ID INT,YEAR_ID INT,PRODUCTLINE STRING,MSRP INT,PRODUCTCODE STRING,
PHONE INT,CITY STRING,STATE  STRING,POSTALCODE INT,COUNTRY STRING,
TERRITORY STRING,CONTACTLASTNAME STRING,CONTACTFIRSTNAME STRING,DEALSIZE STRING)
STORED AS ORC 
TBLPROPERTIES("skip.header.line.count"="1");

 INSERT INTO TABLE sales_order_orc  SELECT * FROM sales_order_csv;
